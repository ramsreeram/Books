
Title		: ASH Scratch Pad 25-Jul-2019, v3a
Author		: Craig A. Shallahamer, craig@orapub.com
Copyright	: (c)OraPub, Inc.
Warrenty	: Absolultey no warrenty. Use at your own risk.

Thank you to everyone who has helped improve this toolkit!

There are five scripts. Each script uses the same core Oracle SQL*Plus DEFINES. The
common defines and common column formats are shown just a little below. The script
specific defines are shown with each script. Obviously, if nothing has changed, then you
do not need to reset the define.

The defines and scripts are created to be copied and pasted directly into SQL*Plus.

The scripts are:

1. Profile Anything - Get a summary of the situation at any level; instance, sql_id,
   session... anything in the source table.

2. Top Anything - Get the top and not so top anything by CPU or WAIT or either; sql_id,
   session, plan_hash_value, event... anything in the source table.
   
3. Timeline Most Anything - Get a sample by sample linear report for a specific session,
   module, sql_id... most anything.
   
4. Tick Report - Get a summary of the entire system grouped by X number of seconds.
   This is like a timeline report, but each line represents all the activity for X seconds.

5. SQL Statement Run Time Inference Report - Infer SQL runtimes using
   the max/min sample_time method.

For a full suite of deep and well formatted ASH based script, check out my FREE
BloodHound Toolkit at www.orapub.com/tools

Check out OraPub for ASH webinars, memberships and ASH Live Video Classroom (LVC) training!


-----------------------------------------------------------------------
Understanding ASH Data
-----------------------------------------------------------------------


set tab off
set verify on
set linesize 300
col minSt format a25
col maxSt format a25

select count(*) from v$active_session_history;

select min(sample_time) minSt, max(sample_time) maxSt from   v$active_session_history;

select distinct session_state, session_type from v$active_session_history;

select sum(decode(session_state,'ON CPU',1,0)) oncpu,
       sum(decode(session_state,'WAITING',1,0)) waiting
from   v$active_session_history;

select sum(decode(session_state,'ON CPU',1,0)) oncpu,
       sum(decode(session_state,'WAITING',1,0)) waiting
from   v$active_session_history
where  sample_time >= current_timestamp - interval '3' minute;


-----------------------------------------------------------------------
Common ASH Scratch Pad Defines
-----------------------------------------------------------------------


All Scratch Pad scripts need the three below defines set; datasource, dbahistdetails
and timing details.

def datasource=v$active_session_history
def datasource=dba_hist_active_sess_history
def datasource=OP_DBA_HIST_ACTV_SESS_HIST

select dbid, instance_number,
       min(sample_time) minSt, max(sample_time) maxSt, count(*)
from   &datasource
group by dbid, instance_number
/

def dbahistdetails=' and dbid=4243270474 and instance_number=3'
def dbahistdetails=' and dbid=1942798052 and instance_number=1'
def dbahistdetails=' and 1=1'

def timingdetails="sample_time >= current_timestamp - interval '1' minute" 

def timingdetails="sample_time between to_date('12-Jul-2014 08:00','DD-Mon-YYYY HH24:MI') and to_date('12-Jul-2014 11:00','DD-Mon-YYYY HH24:MI') "

def timingdetails="sample_time between to_date('05-Sep-2017 11:00','DD-Mon-YYYY HH24:MI') and to_date('05-Sep-2017 14:00','DD-Mon-YYYY HH24:MI')"

def timingdetails="1=1"

---
--- Check to ensure you are looking at the right data
---

set tab off
set verify on
set linesize 300
col minSt format a25
col maxSt format a25

* This will work for both v$ and DBA_HIST ASH data.
*
select min(sample_time) minSt, max(sample_time) maxSt, count(*)
from   &datasource
where  &timingdetails
       &dbahistdetails
/

* This will work ONLY if you are using DBA_HIST like data.
*
select dbid, instance_number,
       min(sample_time) minSt, max(sample_time) maxSt, count(*)
from   &datasource
where  &timingdetails
       &dbahistdetails
group by dbid, instance_number
/


-----------------------------------------------------------------------
Extracting Your Chosen Data
-----------------------------------------------------------------------


To minimize touching a production system, it can be very helpful
to extract production ASH data and then import it into a non-production system.

Also, quickly saving "incident" data can be verfy helpful for later extended analysis
and for educational purposes.


create table myASHextract as
select *
from   &datasource
where  &timingdetails
       &dbahistdetails
/


-----------------------------------------------------------------------
-- Common Column Formats
-----------------------------------------------------------------------


set tab off
set verify on
set linesize 300
col minSt format a25
col maxSt format a25

col sql_id format a15
col session_type format a15
col SESSION_STATE format a7 heading "STATE"
col program format a25 trunc
col module format a15 trunc
col action format a25
col sid format 9999
col event format a25 trunc
col blocking_session heading 'BLK SID'
col session_state format a13
col force_matching_signature format 99999999999999999999
col in_parse heading "P"
col in_hard_parse heading "HP"
col bt format a5  heading "START"
col st format a11 heading "SAMPLE TIME"


-----------------------------------------------------------------------
--- TOP ANYTHING
-----------------------------------------------------------------------


-- What is the top SQL?
-- What is the top SQL when waiting?
-- What is the top session? (w/session_type, program)
-- What is the top session when waiting? (w/session_state,sql_id,event)
-- What is the top force matching signature when the event is...

def ashCols='event,SQL_ID,program'
def ashCols='event,force_matching_signature'
def ashCols='event,module'
def ashCols='event,session_id,session_serial#'
def ashCols='event,session_id,session_type,program,sql_id'
def ashCols='session_state,SQL_ID'
def ashCols='program'
def ashCols='module,action,sql_id'
def ashCols='session_id,session_type,session_state,SQL_ID,event'
def ashCols='session_id,session_state,SQL_ID,event'
def ashCols='session_type,session_id,event'
def ashCols='session_id,session_serial#'
def ashCols='SQL_ID'
def ashCols='SQL_ID,session_id,session_serial#'
def ashCols='event,SQL_ID,session_id,session_serial#'
def ashCols="to_char(sample_time,'DD HH24:MI:SS')"
def ashCols="sql_id,to_char(SQL_EXEC_START,'MI:SS'),sql_exec_id"
def ashCols="sql_id,SQL_PLAN_HASH_VALUE,to_char(SQL_EXEC_START,'MI:SS') DT,sql_exec_id"
def ashCols="sql_id,SQL_PLAN_HASH_VALUE,to_char(SQL_EXEC_START,'MI:SS')||sql_exec_id"
def ashCols="to_char(sample_time,'HH24'),SQL_ID"
def ashCols="in_hard_parse,force_matching_signature"

def ashstate='ON CPU'
def ashstate='WAITING'
def ashstate='%'

def ashWhere='1=1'
def ashWhere="in_hard_parse='Y'"
def ashWhere="event='db file sequential read'"
def ashWhere="sql_id='gaqcyckx7mqws'"

def ashCount="*"

select * from (
  select count(&ashCount) the_count, &ashCols
  from   &datasource
  where  &timingdetails
         &dbahistdetails
    and  session_state like '&ashstate'
    and  &ashWhere
  group by &ashCols
  order by 1 desc, 2 desc
) 
where rownum < 100
/


-----------------------------------------------------------------------
--- TIMELINE
-----------------------------------------------------------------------


-- timeline session 50 (w/event,sql_id,blocking_session)
-- timeline session 170 (w/event,program)

def ashCols='event,sql_id,blocking_session'
def ashCols='event,sql_id,blocking_session,module'
def ashCols='event,sql_id,program'
def ashCols='event,program'
def ashCols='event,sql_id,blocking_session,blocking_inst_id'
def ashCols='event,sql_id,blocking_session,program'
def ashCols='event,sql_id,SQL_PLAN_HASH_VALUE'
def ashCols='event,sql_id,SQL_PLAN_HASH_VALUE,sql_exec_id'
def ashCols="sql_id,event,program,to_char(SQL_EXEC_START,'MI:SS') BT,sql_exec_id,event"
def ashCols="event,sql_id,SQL_PLAN_HASH_VALUE,to_char(SQL_EXEC_START,'MI:SS') BT"
def ashCols="event,sql_id,SQL_PLAN_HASH_VALUE,to_char(SQL_EXEC_START,'MI:SS') BT,sql_exec_id"

def ashWhere='session_id=1589 and session_serial#=52847'
def ashWhere="client_id='COBOL-120-Step-Diaster'"
def ashWhere='session_id=321'
def ashWhere="event like 'enq:%TX%'"
def ashWhere="event like 'enq:%MF%'"
def ashWhere="1=1"

select sample_id, to_char(sample_time,'DD HH24:MI:SS') ST,
       session_id sid, session_state,
       &ashCols
from   &datasource
where  &timingdetails
       &dbahistdetails
  and  &ashWhere
order by 1,3
/


-----------------------------------------------------------------------
--- PROFILE
-----------------------------------------------------------------------


-- Make sure both key/value pairs are set, 1 and 2
--
-- profile all sessions (session_id  %)
-- profile the wait situation for all sessions (session_id   %)
-- profile session 50 (session_id  50)
-- profile session 50 serial # 1234 (session_id 50, session_serial# 1234)
-- profile sql_id (sql_id   abc123)

def ashcolkey1=1
def ashcolval1=1

def ashcolkey1=sql_id
def ashcolval1='7pkuzfhjsgp0f'

def ashcolkey1=module
def ashcolval1='inventory'

def ashcolkey1=session_id
def ashcolval1=2177

def ashcolkey2=1
def ashcolval2=1

def ashcolkey2=session_serial#
def ashcolval2=12301


set verify off
col totAS heading "Total|AS"
col cpuPct format 990.0 heading "CPU|PCT"
col oncpuinparseX heading "CPU|PARSING PCT" format 990.0
col oncpuinhardparseX heading "CPU HARD|PARSING PCT" format 990.0
col waitingPct format 990.0 heading "WAITING|PCT"
col ioPct format 990.0 heading "WAITING|IO PCT"
col otherPct format 990.0 heading "WAITING|OTHER PCT"
--
-- Summary with CPU parsing details
--
select totAS,
       100*oncpu/(oncpu+waiting) cpuPct,
       100*oncpuinparse/oncpu oncpuinparseX,
       100*oncpuinhardparse/oncpu oncpuinhardparseX,
       100*waiting/(oncpu+waiting) waitingPct,
       100*(userIO+sysIO)/waiting ioPct,
       100*(waiting-userIO-sysIO)/waiting otherPct
from
(
  select 
    count(*) totAS,
    sum(decode(session_state,'ON CPU',1,0)) oncpu,
    sum(decode(session_state,'ON CPU',decode(in_parse,'Y',1,0))) oncpuinparse,
    sum(decode(session_state,'ON CPU',decode(in_hard_parse,'Y',1,0))) oncpuinhardparse,
    sum(decode(session_state,'WAITING',1,0)) waiting,
    sum(decode(session_state,'WAITING',decode(wait_class,'User I/O',1,0))) userIO,
    sum(decode(session_state,'WAITING',decode(wait_class,'System I/O',1,0))) sysIO
  from   &datasource
  where  &timingdetails
         &dbahistdetails
    and  &ashcolkey1 like '&ashcolval1%'
    and  &ashcolkey2 like '&ashcolval2%'
)
/
--
-- WAIT Details
--
select count(*), event
from   &datasource
where  &timingdetails
       &dbahistdetails
  and  session_state = 'WAITING'
  and  &ashcolkey1 like '&ashcolval1%'
  and  &ashcolkey2 like '&ashcolval2%'
group by event
order by 1 desc
/


-----------------------------------------------------------------------
--- TICK REPORT : Inteval Timeline Report
-----------------------------------------------------------------------


-- For a more robust tick report, use the BloodHound Toolkit.

set pagesize 100

def group_sec=10

col trunc_group_seconds noprint
col cpu_pct format 999.0
col wait_pct format 999.0
col event format a45
col min_sample_dt format a20
col tas format 9990.0
col aas format 9990.0

select	a.trunc_group_seconds, a.min_sample_dt, a.min_sample_id, a.TAS, a.AAS,
	100*a.cpu_count/(a.cpu_count+a.wait_count) cpu_pct, 100*a.wait_count/(a.cpu_count+a.wait_count) wait_pct,
	b.event
from	(
	select	trunc_group_seconds, min(sample_dt) min_sample_dt, min(sample_id) min_sample_id, 
		count(*) TAS,
		count(distinct(sample_id)) tot_sample_ids,
		round(count(*)/count(distinct(sample_id)),1) AAS,
		sum(decode(session_state,'ON CPU',1,0)) cpu_count,
		sum(decode(session_state,'WAITING',1,0)) wait_count
	from	(
		select	to_char(sample_time,'YYYY-Mon-DD HH24:MI:SS') sample_dt,
			to_char(sample_time,'DDD')*24*60*60+to_char(sample_time,'SSSSS') sample_time_in_seconds,
			trunc(to_char(sample_time,'DDD')*24*60*60+to_char(sample_time,'SSSSS')/&group_sec) trunc_group_seconds,
			sample_id,
			session_id,
			session_state
		from	&datasource
		where	&timingdetails
			&dbahistdetails
		--where rownum < 200
		order by 1
		)
	group by trunc_group_seconds 
	) a,
	(
	select	trunc_group_seconds, event
	from	(
		select	trunc_group_seconds, event,
			rank() over (partition by trunc_group_seconds order by cnt desc) the_rank
		from	(
			select	trunc(to_char(sample_time,'DDD')*24*60*60+to_char(sample_time,'SSSSS')/&group_sec) trunc_group_seconds,
				event,
				count(*) cnt
			from	&datasource
			where	&timingdetails
				&dbahistdetails
			  and	session_state = 'WAITING'
			group by trunc(to_char(sample_time,'DDD')*24*60*60+to_char(sample_time,'SSSSS')/&group_sec),
				 event
			)
		)
	where	the_rank = 1
	--order by 1
	) b
where a.trunc_group_seconds = b.trunc_group_seconds
order by a.min_sample_id
/


-----------------------------------------------------------------------
--- SQL Statement Run Time Inference Report
-----------------------------------------------------------------------

-- comma delimited output for R and machine learning analysis
--
set heading off
set verify off
col start_date_time noprint
col sql_exec_id noprint
def mydelim=','

def mysql_id='b62zukh71caj7'

-- standard column display
--
set heading on
set verify on
col start_date_time print
col sql_exec_id print
def mydelim=''


col runtime_sec format 99999990
select  start_date_time, sql_exec_id,
        abs(extract( day from runtime ))*24*60 +
        abs(extract( hour from runtime ))*60 +
        abs(extract( minute from runtime ))*60 +
        abs(extract( second from runtime ))||'&mydelim' runtime_sec
from (
   select  to_timestamp_tz(max(sample_time))-to_timestamp_tz(min(sample_time)) runtime,
           to_char(SQL_EXEC_START,'YYYY-Mon-DD HH24:MI:SS') start_date_time,
           sql_exec_id
    from   &datasource
	where  &timingdetails
		   &dbahistdetails
      and  sql_id='&mysql_id'
    group by to_char(SQL_EXEC_START,'YYYY-Mon-DD HH24:MI:SS'), sql_exec_id
)
order by 1,2
/

Example Output:

START_DATE_TIME      SQL_EXEC_ID RUNTIME_SEC
-------------------- ----------- ----------------------------------------
2019-Feb-02 13:40:41    16777216 50.142
2019-Feb-09 13:50:30    16777216 50.202
2019-Feb-16 14:05:22    16777216 60.21
2019-Jan-05 14:00:07    16777216 40.17
2019-Jan-12 14:02:08    16777216 60.206
2019-Jan-19 14:21:59    16777216 100.35
2019-Jan-26 14:01:29    16777216 100.354

--END


